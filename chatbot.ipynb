{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhwF8CVzPDjf",
        "outputId": "2fb4d684-395c-4ca4-838c-8ceb3097bce9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.16.0\n",
            "  Downloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.0) (2.32.3)\n",
            "Collecting torch==2.1.0 (from torchtext==0.16.0)\n",
            "  Downloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl.metadata (25 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.16.0) (1.26.4)\n",
            "Collecting torchdata==0.7.0 (from torchtext==0.16.0)\n",
            "  Downloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0->torchtext==0.16.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->torchtext==0.16.0)\n",
            "  Downloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.7.0->torchtext==0.16.0) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->torchtext==0.16.0) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.16.0) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0->torchtext==0.16.0) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0->torchtext==0.16.0) (1.3.0)\n",
            "Downloading torchtext-0.16.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch-2.1.0-cp311-cp311-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchdata-0.7.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m25.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchdata, torchtext\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.1.0\n",
            "    Uninstalling triton-3.1.0:\n",
            "      Successfully uninstalled triton-3.1.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.1+cu124\n",
            "    Uninstalling torch-2.5.1+cu124:\n",
            "      Successfully uninstalled torch-2.5.1+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.20.1+cu124 requires torch==2.5.1, but you have torch 2.1.0 which is incompatible.\n",
            "torchaudio 2.5.1+cu124 requires torch==2.5.1, but you have torch 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvtx-cu12-12.1.105 torch-2.1.0 torchdata-0.7.0 torchtext-0.16.0 triton-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.16.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rEXcHtuYPEUz"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FYNnbjlPHZ3",
        "outputId": "aa01c186-eabe-4c40-93ce-260c01a8e006"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package abc is already up-to-date!\n",
            "[nltk_data]    | Downloading package alpino to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package alpino is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_rus is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package bcp47 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bcp47 is already up-to-date!\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown is already up-to-date!\n",
            "[nltk_data]    | Downloading package brown_tei to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_cat to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
            "[nltk_data]    | Downloading package cess_esp to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
            "[nltk_data]    | Downloading package chat80 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package chat80 is already up-to-date!\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package city_database is already up-to-date!\n",
            "[nltk_data]    | Downloading package cmudict to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package comtrans to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package comtrans is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2000 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2002 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
            "[nltk_data]    | Downloading package conll2007 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
            "[nltk_data]    | Downloading package crubadan to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package crubadan is already up-to-date!\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package dolch to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package dolch is already up-to-date!\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package english_wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package extended_omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package floresta to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package floresta is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package ieer to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ieer is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package indian to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package indian is already up-to-date!\n",
            "[nltk_data]    | Downloading package jeita to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package jeita is already up-to-date!\n",
            "[nltk_data]    | Downloading package kimmo to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package kimmo is already up-to-date!\n",
            "[nltk_data]    | Downloading package knbc to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package knbc is already up-to-date!\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
            "[nltk_data]    | Downloading package mac_morpho to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
            "[nltk_data]    | Downloading package machado to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package machado is already up-to-date!\n",
            "[nltk_data]    | Downloading package masc_tagged to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker_tab is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package maxent_treebank_pos_tagger_tab is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package mte_teip5 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
            "[nltk_data]    | Downloading package mwa_ppdb to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package nombank.1.0 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
            "[nltk_data]    | Downloading package nps_chat to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package paradigms to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package paradigms is already up-to-date!\n",
            "[nltk_data]    | Downloading package pe08 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pe08 is already up-to-date!\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
            "[nltk_data]    | Downloading package pil to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pil is already up-to-date!\n",
            "[nltk_data]    | Downloading package pl196x to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pl196x is already up-to-date!\n",
            "[nltk_data]    | Downloading package porter_test to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package porter_test is already up-to-date!\n",
            "[nltk_data]    | Downloading package ppattach to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ppattach is already up-to-date!\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package propbank to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package propbank is already up-to-date!\n",
            "[nltk_data]    | Downloading package pros_cons to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
            "[nltk_data]    | Downloading package ptb to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ptb is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package qc is already up-to-date!\n",
            "[nltk_data]    | Downloading package reuters to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package reuters is already up-to-date!\n",
            "[nltk_data]    | Downloading package rslp to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rslp is already up-to-date!\n",
            "[nltk_data]    | Downloading package rte to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package rte is already up-to-date!\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package semcor to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package semcor is already up-to-date!\n",
            "[nltk_data]    | Downloading package senseval to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package senseval is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package smultron to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package smultron is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
            "[nltk_data]    | Downloading package state_union to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package state_union is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
            "[nltk_data]    | Downloading package swadesh to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package swadesh is already up-to-date!\n",
            "[nltk_data]    | Downloading package switchboard to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package switchboard is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets is already up-to-date!\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package tagsets_json is already up-to-date!\n",
            "[nltk_data]    | Downloading package timit to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package timit is already up-to-date!\n",
            "[nltk_data]    | Downloading package toolbox to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package toolbox is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr is already up-to-date!\n",
            "[nltk_data]    | Downloading package udhr2 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
            "[nltk_data]    |       date!\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package verbnet3 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
            "[nltk_data]    | Downloading package webtext to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package webtext is already up-to-date!\n",
            "[nltk_data]    | Downloading package wmt15_eval to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2022 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet2022 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package ycoe to\n",
            "[nltk_data]    |     C:\\Users\\ADMIN\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]    |   Package ycoe is already up-to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Tải dữ liệu cần thiết của nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "8cUr2My_PH0e"
      },
      "outputs": [],
      "source": [
        "# Tiền xử lý dữ liệu\n",
        "def preprocess_text(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ZzzTZ1kVPLH-"
      },
      "outputs": [],
      "source": [
        "# Đọc dữ liệu từ file CSV\n",
        "def load_data(csv_file):\n",
        "    df = pd.read_csv(csv_file)\n",
        "    df.dropna(inplace=True)\n",
        "    questions = df['question'].tolist()\n",
        "    answers = df['answer'].tolist()\n",
        "    return questions, answers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "6SJefoDRPMwU"
      },
      "outputs": [],
      "source": [
        "# Load dữ liệu từ file CSV\n",
        "csv_file = \"qa.csv\"\n",
        "questions, answers = load_data(csv_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "A1Z-5JVpPOhg"
      },
      "outputs": [],
      "source": [
        "# Tiền xử lý câu hỏi\n",
        "processed_questions = [preprocess_text(q) for q in questions]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "4CsYzo__PPAV"
      },
      "outputs": [],
      "source": [
        "# Tạo từ điển vocab\n",
        "def build_vocab(tokenized_texts):\n",
        "    word_freq = Counter([word for text in tokenized_texts for word in text])\n",
        "    word_vocab = vocab(word_freq, specials=[\"<pad>\", \"<unk>\"])\n",
        "    word_vocab.set_default_index(word_vocab[\"<unk>\"])\n",
        "    return word_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k0nRuLtFPSob"
      },
      "outputs": [],
      "source": [
        "word_vocab = build_vocab(processed_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "BND5ssBHPUKR"
      },
      "outputs": [],
      "source": [
        "# Chuyển câu hỏi thành vector số\n",
        "def numericalize(text, vocab):\n",
        "    return [vocab[word] for word in text]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "dcWmw7a0PWIa"
      },
      "outputs": [],
      "source": [
        "numerical_questions = [numericalize(q, word_vocab) for q in processed_questions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Xq3lXbwWPZoY"
      },
      "outputs": [],
      "source": [
        "# Padding dữ liệu\n",
        "def pad_sequences(sequences, max_len):\n",
        "    return [seq + [word_vocab[\"<pad>\"]] * (max_len - len(seq)) if len(seq) < max_len else seq[:max_len] for seq in sequences]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "ygVUrcDnPbTc"
      },
      "outputs": [],
      "source": [
        "max_len = max(len(q) for q in numerical_questions)\n",
        "numerical_questions = pad_sequences(numerical_questions, max_len)\n",
        "numerical_questions = torch.tensor(numerical_questions)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "98"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "964_EgsBPdLX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, max_len, dropout=0.1):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=word_vocab[\"<pad>\"])\n",
        "        self.positional_encoding = self._generate_positional_encoding(max_len, embed_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(embed_dim, num_heads, hidden_dim, dropout)\n",
        "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
        "\n",
        "        self.decoder = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _generate_positional_encoding(self, max_len, embed_dim):\n",
        "        pe = torch.zeros(max_len, embed_dim)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_dim))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        return pe.unsqueeze(0)  # (1, max_len, embed_dim)\n",
        "\n",
        "    def forward(self, x, src_mask=None):\n",
        "        x = self.embedding(x) + self.positional_encoding[:, :x.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        x = self.encoder(x, src_mask)\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def generate_text(self, x, max_len=10):\n",
        "        outputs = []\n",
        "        for _ in range(max_len):\n",
        "            x = self.forward(x).argmax(dim=-1)\n",
        "            outputs.append(x)\n",
        "            if (x == word_vocab[\"<pad>\"]).all():\n",
        "                break  # Dừng nếu toàn bộ câu là <pad>\n",
        "        return torch.cat(outputs, dim=1)  # (batch_size, max_len)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YbffvlDhPeuP",
        "outputId": "44416a6b-dab0-42a5-e8e6-584abef66fb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\AIO2023\\myenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ],
      "source": [
        "# Tạo mô hình\n",
        "vocab_size = len(word_vocab)\n",
        "model = SimpleTransformer(vocab_size, embed_dim=64, num_heads=4, hidden_dim=128, num_layers=2, max_len=max_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28XNGb8aQUFW",
        "outputId": "40e85c5c-257a-43cd-9eb7-0cd25b95697c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-4bd9aaeb0726>:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  q_tensor = torch.tensor(q).unsqueeze(0)  # Thêm batch dimension\n",
            "<ipython-input-16-4bd9aaeb0726>:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  a_tensor = torch.tensor(a).unsqueeze(0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 5.5709\n",
            "Epoch 2, Loss: 5.0746\n",
            "Epoch 3, Loss: 4.9828\n",
            "Epoch 4, Loss: 4.9167\n",
            "Epoch 5, Loss: 4.8827\n",
            "Epoch 6, Loss: 4.8495\n",
            "Epoch 7, Loss: 4.8323\n",
            "Epoch 8, Loss: 4.8138\n",
            "Epoch 9, Loss: 4.7990\n",
            "Epoch 10, Loss: 4.7816\n",
            "Epoch 11, Loss: 4.7673\n",
            "Epoch 12, Loss: 4.7550\n",
            "Epoch 13, Loss: 4.7510\n",
            "Epoch 14, Loss: 4.7487\n",
            "Epoch 15, Loss: 4.7472\n",
            "Epoch 16, Loss: 4.7312\n",
            "Epoch 17, Loss: 4.7396\n",
            "Epoch 18, Loss: 4.7285\n",
            "Epoch 19, Loss: 4.7045\n",
            "Epoch 20, Loss: 4.7039\n",
            "Epoch 21, Loss: 4.7030\n",
            "Epoch 22, Loss: 4.6953\n",
            "Epoch 23, Loss: 4.6848\n",
            "Epoch 24, Loss: 4.6941\n",
            "Epoch 25, Loss: 4.6737\n",
            "Epoch 26, Loss: 4.7169\n",
            "Epoch 27, Loss: 4.6850\n",
            "Epoch 28, Loss: 4.6987\n",
            "Epoch 29, Loss: 4.7107\n",
            "Epoch 30, Loss: 4.6857\n",
            "Epoch 00031: reducing learning rate of group 0 to 5.0000e-04.\n",
            "Epoch 31, Loss: 4.7581\n",
            "Epoch 32, Loss: 4.7657\n",
            "Epoch 33, Loss: 5.0908\n",
            "Epoch 34, Loss: 5.0413\n",
            "Epoch 35, Loss: 5.0503\n",
            "Early stopping at epoch 35\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "def train_model(model, questions, answers, vocab, epochs=100, lr=0.001, patience=10):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)  # Thêm weight decay\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=vocab[\"<pad>\"])\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    counter = 0  # Biến đếm cho early stopping\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        model.train()  # Đặt mô hình ở chế độ train\n",
        "\n",
        "        for q, a in zip(questions, answers):\n",
        "            q_tensor = torch.tensor(q).unsqueeze(0)  # Thêm batch dimension\n",
        "            a_tensor = torch.tensor(a).unsqueeze(0)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(q_tensor)  # Dự đoán\n",
        "            loss = loss_fn(output.view(-1, len(vocab)), a_tensor.view(-1))  # Tính loss\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Gradient Clipping\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(questions)\n",
        "        scheduler.step(avg_loss)  # Giảm learning rate nếu cần\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Early Stopping\n",
        "        if avg_loss < best_loss:\n",
        "            best_loss = avg_loss\n",
        "            counter = 0\n",
        "        else:\n",
        "            counter += 1\n",
        "            if counter >= patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "# Tiền xử lý câu trả lời thành số\n",
        "processed_answers = [preprocess_text(a) for a in answers]\n",
        "numerical_answers = [numericalize(a, word_vocab) for a in processed_answers]\n",
        "numerical_answers = pad_sequences(numerical_answers, max_len)\n",
        "numerical_answers = torch.tensor(numerical_answers)\n",
        "\n",
        "# Huấn luyện mô hình\n",
        "train_model(model, numerical_questions, numerical_answers, word_vocab, epochs=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bLqja5UbQWGj"
      },
      "outputs": [],
      "source": [
        "def get_answer(question, top_k=3, threshold=0.5):\n",
        "    processed_q = preprocess_text(question)\n",
        "    q_vector = numericalize(processed_q, word_vocab)\n",
        "    q_vector = pad_sequences([q_vector], max_len)\n",
        "    q_vector = torch.tensor(q_vector)\n",
        "\n",
        "    # Tính cosine similarity\n",
        "    similarities = cosine_similarity(q_vector.numpy(), numerical_questions.numpy())\n",
        "    top_k_indices = np.argsort(similarities[0])[-top_k:][::-1]\n",
        "\n",
        "    # Nếu có câu hỏi giống trên ngưỡng threshold, lấy câu trả lời từ dữ liệu\n",
        "    best_idx = top_k_indices[0]\n",
        "    if similarities[0][best_idx] >= threshold:\n",
        "        best_answer = answers[best_idx]\n",
        "        similar_questions = [questions[idx] for idx in top_k_indices]\n",
        "    else:\n",
        "        # Dùng mô hình Transformer để sinh câu trả lời\n",
        "        with torch.no_grad():\n",
        "            output = model(q_vector).squeeze(0)  # (max_len, vocab_size)\n",
        "            predicted_indices = output.argmax(dim=1).tolist()\n",
        "            best_answer = \" \".join([word for word, idx in word_vocab.get_stoi().items() if idx in predicted_indices])\n",
        "\n",
        "        # Dùng mô hình Transformer để tự sinh 3 câu hỏi tương tự\n",
        "        with torch.no_grad():\n",
        "            generated_questions = model.generate_text(q_vector, max_len=10)\n",
        "            similar_questions = []\n",
        "            for gen_q in generated_questions:\n",
        "                text = \" \".join([word for word, idx in word_vocab.get_stoi().items() if idx in gen_q.tolist()])\n",
        "                similar_questions.append(text)\n",
        "\n",
        "    return best_answer, similar_questions[:3]  # Trả về 3 câu hỏi tự sinh\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m3ymBT8QvBh",
        "outputId": "f6c1ecc4-ba36-46fa-d689-3a40afec0955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Câu trả lời: Only individuals with a valid driving license with remaining points can rent a self-drive vehicle.\n",
            "3 câu hỏi tương tự tự sinh: ['Who can rent a vehicle for self-driving?', 'What should I do if I am involved in an accident?', 'What is an internal road?']\n"
          ]
        }
      ],
      "source": [
        "answer, similar_questions = get_answer(\"Who can rent a vehicle for self-driving?\")\n",
        "print(\"Câu trả lời:\", answer)\n",
        "print(\"3 câu hỏi tương tự tự sinh:\", similar_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kmOd_WrPmcn",
        "outputId": "50acc8e6-41d8-4f1e-bf27-a2f491743de0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model saved to simple_transformer.pth\n"
          ]
        }
      ],
      "source": [
        "# Lưu mô hình\n",
        "model_path = \"simple_transformer.pth\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
